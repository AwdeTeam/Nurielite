{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Machine Learning Development</h1>\n",
    "<p>This page is dedicated to developing and exploring the execution of ML algorithms using <code>Numpy</code> and <code>Scipy</code> along with <code>Matplotlib</code> and <code>QT Python</code> as display tools.</p>\n",
    "<p>When possible, try to separate the actual implementation into a single cell and a single class to minimize difficulty when translating to <code>PyAlgorithm</code> interface form.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center;\">K-Means</h2>\n",
    "\n",
    "<p><h3><a href=\"https://en.wikipedia.org/wiki/K-means_clustering\"><a name=\"k_means\" /><i>k</i>-means clustering</a></h3> is a method of <a href=\"#vector_quantization\">vector quantization</a>, originally from <a href=\"https://en.wikipedia.org/wiki/Signal_processing\">signal processing</a>, that is popular for <a href=\"#cluster_analysis\">cluster analysis</a> in data mining. <i>k</i>-means clustering aims to <a href=\"/wiki/Partition_of_a_set\">partition</a> <i>n</i> observations into <i>k</i> clusters in which each observation belongs to the <a href=\"/wiki/Cluster_(statistics)\" class=\"mw-redirect\" title=\"Cluster (statistics)\">cluster</a> with the nearest <a href=\"/wiki/Mean\" title=\"Mean\">mean</a>, serving as a <a href=\"/wiki/Prototype\" title=\"Prototype\">prototype</a> of the cluster. This results in a partitioning of the data space into <a href=\"#voronoi\">Voronoi cells</a>.</p>\n",
    "<p>The problem is computationally difficult (<a href=\"/wiki/NP-hard\" class=\"mw-redirect\" title=\"NP-hard\">NP-hard</a>); however, there are efficient <a href=\"/wiki/Heuristic_algorithm\" class=\"mw-redirect\" title=\"Heuristic algorithm\">heuristic algorithms</a> that are commonly employed and converge quickly to a <a href=\"/wiki/Local_optimum\" title=\"Local optimum\">local optimum</a>. These are usually similar to the <a href=\"/wiki/Expectation-maximization_algorithm\" class=\"mw-redirect\" title=\"Expectation-maximization algorithm\">expectation-maximization algorithm</a> for <a href=\"/wiki/Mixture_model\" title=\"Mixture model\">mixtures</a> of <a href=\"/wiki/Gaussian_distribution\" class=\"mw-redirect\" title=\"Gaussian distribution\">Gaussian distributions</a> via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, <i>k</i>-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.</p>\n",
    "<p>The algorithm has a loose relationship to the <a href=\"/wiki/K-nearest_neighbor\" class=\"mw-redirect\" title=\"K-nearest neighbor\"><i>k</i>-nearest neighbor classifier</a>, a popular <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a> technique for classification that is often confused with <i>k</i>-means because of the <i>k</i> in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by <i>k</i>-means to classify new data into the existing clusters. This is known as <a href=\"/wiki/Nearest_centroid_classifier\" title=\"Nearest centroid classifier\">nearest centroid classifier</a> or Rocchio algorithm.</p>\n",
    "\n",
    "<p><h3><a name=\"vector_quantization\"></a><a href=\"https://en.wikipedia.org/wiki/Vector_quantization\">Vector quantization</a></h3>(VQ) is a classical <a href=\"/wiki/Quantization_(signal_processing)\" title=\"Quantization (signal processing)\">quantization</a> technique from <a href=\"/wiki/Signal_processing\" title=\"Signal processing\">signal processing</a> that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for <a href=\"/wiki/Data_compression\" title=\"Data compression\">data compression</a>. It works by dividing a large set of vectors into groups having approximately the same number of points closest to them. Each group is represented by its <a href=\"/wiki/Centroid\" title=\"Centroid\">centroid</a> point, as in <a href=\"#k_means\">k-means</a> and some other <a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">clustering</a> algorithms.</p>\n",
    "<p>The density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for <a href=\"/wiki/Lossy_data_compression\" class=\"mw-redirect\" title=\"Lossy data compression\">lossy data compression</a>. It can also be used for lossy data correction and <a href=\"/wiki/Density_estimation\" title=\"Density estimation\">density estimation</a>.</p>\n",
    "<p>Vector quantization is based on the <a href=\"/wiki/Competitive_learning\" title=\"Competitive learning\">competitive learning</a> paradigm, so it is closely related to the <a href=\"/wiki/Self-organizing_map\" title=\"Self-organizing map\">self-organizing map</a> model and to <a href=\"/wiki/Sparse_coding\" class=\"mw-redirect\" title=\"Sparse coding\">sparse coding</a> models used in <a href=\"/wiki/Deep_learning\" title=\"Deep learning\">deep learning</a> algorithms such as <a href=\"/wiki/Autoencoder\" title=\"Autoencoder\">autoencoder</a>.</p>\n",
    "\n",
    "<p><a name=\"#cluster_analysis\"></a><h3><a href=\"https://en.wikipedia.org/wiki/Cluster_analysis\">Cluster analysis</a></h3> or <b>clustering</b> is the task of grouping a set of objects in such a way that objects in the same group (called a <b><span id=\"cluster\">cluster</span></b>) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory <a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a>, and a common technique for <a href=\"/wiki/Statistics\" title=\"Statistics\">statistical</a> <a href=\"/wiki/Data_analysis\" title=\"Data analysis\">data analysis</a>, used in many fields, including <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a>, <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a>, <a href=\"/wiki/Image_analysis\" title=\"Image analysis\">image analysis</a>, <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a>, <a href=\"/wiki/Bioinformatics\" title=\"Bioinformatics\">bioinformatics</a>, <a href=\"/wiki/Data_compression\" title=\"Data compression\">data compression</a>, and <a href=\"/wiki/Computer_graphics\" title=\"Computer graphics\">computer graphics</a>.</p>\n",
    "<p>Cluster analysis itself is not one specific <a href=\"/wiki/Algorithm\" title=\"Algorithm\">algorithm</a>, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small <a href=\"/wiki/Distance_function\" class=\"mw-redirect\" title=\"Distance function\">distances</a> among the cluster members, dense areas of the data space, intervals or particular <a href=\"/wiki/Statistical_distribution\" class=\"mw-redirect\" title=\"Statistical distribution\">statistical distributions</a>. Clustering can therefore be formulated as a <a href=\"/wiki/Multi-objective_optimization\" title=\"Multi-objective optimization\">multi-objective optimization</a> problem. The appropriate clustering algorithm and parameter settings (including values such as the <a href=\"/wiki/Metric_(mathematics)\" title=\"Metric (mathematics)\">distance function</a> to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of <a href=\"/wiki/Knowledge_discovery\" class=\"mw-redirect\" title=\"Knowledge discovery\">knowledge discovery</a> or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.</p>\n",
    "<p>Besides the term <i>clustering</i>, there are a number of terms with similar meanings, including <i>automatic <a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">classification</a></i>, <i><a href=\"/wiki/Numerical_taxonomy\" title=\"Numerical taxonomy\">numerical taxonomy</a></i>, <i>botryology</i> (from Greek βότρυς \"grape\") and <i>typological analysis</i>. The subtle differences are often in the usage of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.</p>\n",
    "<p>Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Zubin in 1938 and <a href=\"/wiki/Robert_Tryon\" title=\"Robert Tryon\">Robert Tryon</a> in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.</p>\n",
    "\n",
    "<h3><a name=\"voronoi\"></a><a href=\"https://en.wikipedia.org/wiki/Voronoi_diagram\">Voronoi Diagram</a></h3>\n",
    "<p>In mathematics, a <b>Voronoi diagram</b> is a <a href=\"/wiki/Partition_of_a_set\" title=\"Partition of a set\">partitioning</a> of a <a href=\"/wiki/Plane_(geometry)\" title=\"Plane (geometry)\">plane</a> into regions based on distance to points in a specific subset of the plane. That set of points (called seeds, sites, or generators) is specified beforehand, and for each seed there is a corresponding region consisting of all points closer to that seed than to any other. These regions are called Voronoi cells. The Voronoi diagram of a set of points is <a href=\"/wiki/Duality_(mathematics)\" title=\"Duality (mathematics)\">dual</a> to its <a href=\"/wiki/Delaunay_triangulation\" title=\"Delaunay triangulation\">Delaunay triangulation</a>.</p>\n",
    "<p>It is named after <a href=\"/wiki/Georgy_Voronoi\" class=\"mw-redirect\" title=\"Georgy Voronoi\">Georgy Voronoi</a>, and is also called a <b>Voronoi tessellation</b>, a <b>Voronoi decomposition</b>, a <b>Voronoi partition</b>, or a <b>Dirichlet tessellation</b> (after <a href=\"/wiki/Peter_Gustav_Lejeune_Dirichlet\" title=\"Peter Gustav Lejeune Dirichlet\">Peter Gustav Lejeune Dirichlet</a>). Voronoi diagrams have practical and theoretical applications to a large number of fields, mainly in science and technology but also including visual art.</p>\n",
    "\n",
    "<h3><a name=\"gramschmidt\"></a><a href=\"https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process\">Gram-Schmidt Process</a></h3>\n",
    "<p>In <a href=\"/wiki/Mathematics\" title=\"Mathematics\">mathematics</a>, particularly <a href=\"/wiki/Linear_algebra\" title=\"Linear algebra\">linear algebra</a> and <a href=\"/wiki/Numerical_analysis\" title=\"Numerical analysis\">numerical analysis</a>, the <b>Gram–Schmidt process</b> is a method for <a href=\"/wiki/Orthonormal_basis\" title=\"Orthonormal basis\">orthonormalising</a> a set of <a href=\"/wiki/Vector_(geometry)\" class=\"mw-redirect\" title=\"Vector (geometry)\">vectors</a> in an <a href=\"/wiki/Inner_product_space\" title=\"Inner product space\">inner product space</a>, most commonly the <a href=\"/wiki/Euclidean_space\" title=\"Euclidean space\">Euclidean space</a> <b>R</b><sup><i>n</i></sup> equipped with the <a href=\"/wiki/Standard_inner_product\" class=\"mw-redirect\" title=\"Standard inner product\">standard inner product</a>. The Gram–Schmidt process takes a <a href=\"/wiki/Finite_set\" title=\"Finite set\">finite</a>, <a href=\"/wiki/Linearly_independent\" class=\"mw-redirect\" title=\"Linearly independent\">linearly independent</a> set <i>S</i> = {<i>v</i><sub>1</sub>, ..., <i>v</i><sub><i>k</i></sub>} for <span class=\"nowrap\"><i>k</i> ≤ <i>n</i></span> and generates an <a href=\"/wiki/Orthogonal_set\" class=\"mw-redirect\" title=\"Orthogonal set\">orthogonal set</a> <span class=\"nowrap\"><i>S′</i> = {<i>u</i><sub>1</sub>, ..., <i>u</i><sub><i>k</i></sub>}</span> that spans the same <i>k</i>-dimensional subspace of <b>R</b><sup><i>n</i></sup> as <i>S</i>.</p>\n",
    "<p>The method is named after <a href=\"/wiki/J%C3%B8rgen_Pedersen_Gram\" title=\"Jørgen Pedersen Gram\">Jørgen Pedersen Gram</a> and <a href=\"/wiki/Erhard_Schmidt\" title=\"Erhard Schmidt\">Erhard Schmidt</a> but it appeared earlier in the work of <a href=\"/wiki/Pierre-Simon_Laplace\" title=\"Pierre-Simon Laplace\">Laplace</a> and <a href=\"/wiki/Augustin-Louis_Cauchy\" title=\"Augustin-Louis Cauchy\">Cauchy</a>. In the theory of <a href=\"/wiki/Lie_group_decompositions\" class=\"mw-redirect\" title=\"Lie group decompositions\">Lie group decompositions</a> it is generalized by the <a href=\"/wiki/Iwasawa_decomposition\" title=\"Iwasawa decomposition\">Iwasawa decomposition</a>.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">[1]</a></sup></p>\n",
    "<p>The application of the Gram–Schmidt process to the column vectors of a full column <a href=\"/wiki/Rank_(linear_algebra)\" title=\"Rank (linear algebra)\">rank</a> <a href=\"/wiki/Matrix_(mathematics)\" title=\"Matrix (mathematics)\">matrix</a> yields the <a href=\"/wiki/QR_decomposition\" title=\"QR decomposition\">QR decomposition</a> (it is decomposed into an <a href=\"/wiki/Orthogonal_matrix\" title=\"Orthogonal matrix\">orthogonal</a> and a <a href=\"/wiki/Triangular_matrix\" title=\"Triangular matrix\">triangular matrix</a>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Algorithm and Description</h2><a name=\"k_means_alg_math\"></a>\n",
    "<p>Given a set of observations ($\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_n}$), where each observation is a <i>d</i>-dimensional real vector, <i>k</i>-means clustering aims to partition the <i>n</i> observations into <i>k</i> (≤ <i>n</i>) sets $\\mathbf{S} = \\{S_1, S_2, \\dots, S_k\\}$ so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center). In other words, its objective is to find:</p>\n",
    "\n",
    "$${\\underset {\\mathbf {S} }{\\operatorname {arg\\,min} }}\\sum _{i=1}^{k}\\sum _{\\mathbf {x} \\in S_{i}}\\left\\|\\mathbf {x} -{\\boldsymbol {\\mu }}_{i}\\right\\|^{2}$$\n",
    "\n",
    "<p>where $\\mathbf{\\mu_i}$ is the mean of points in $S_i$.</p>\n",
    "\n",
    "<h3>Voronoi Algorithm</h3>\n",
    "Basically, Voronoi takes a metric n-space $\\mathbf{X}^n$ and a set of vectors $\\vec{v_1}, \\vec{v_2}, \\dots{v_m}\\in\\mathbf{X}^n$ and finds regions $i$ defined by points closer to a given vector $\\vec{v_i}$.<br>\n",
    "<h4>The Euclidean n-dimensional Voronoi</h4>\n",
    "The first problem to solve is how we divide $\\mathbf{X}^n$ into two parts based off of two vectors, $\\vec{v_1}$ and $\\vec{v_2}$.\n",
    "The obvious way to do this is to use an $n-1$ dimensional surface (a <a href=\"https://en.wikipedia.org/wiki/Hyperplane\">hyperplane</a>).\n",
    "We can construct this surface as follows:\n",
    "$$\\vec{v_c} = \\frac{\\vec{v_1}-\\vec{v_2}}{2}+\\vec{v_1}$$\n",
    "This builds a vector that points from $\\vec{v_1}$ to a point halfway between $\\vec{v_1}$ and $\\vec{v_2}$.\n",
    "Using this vector as $\\vec{u_1}$, we calculate the <a href=\"#gramschmidt\">Gram-Schmidt</a> orthonormal basis for the objective hyperplane $h_{1,2}$ as defined by $\\vec{u_2},\\dots,\\vec{u_n}$.\n",
    "\n",
    "(note, this is super ineffecient and generates every hyperplane twice an a lot of hyperplanes that are never used, fix pls;\n",
    "maybe define regions based on where hyperplanes intersect?)<br>\n",
    "Defining a region $R_i$ now as the union of hyperplanes $\\bigcup_{j=1}^{n} h_{i,j}$, we say that a vector $\\vec{p}$ is inside $R_i$ if $\\overrightarrow{\\vec{v_i}\\vec{p}}\\cap R_i=\\emptyset$.\n",
    "\n",
    "(note, since all we need from the plane is a point and a normal, we may just need to collect $\\vec{v_c}$ instead of the entire orthonormal set)\n",
    "<h4><a href=\"https://en.wikipedia.org/wiki/Line%E2%80%93plane_intersection\">Line-Plane Intersection</a></h4>\n",
    "TO determine whether a hyperplane $h$ lies between two points, $\\vec{p_1}$ and $\\vec{p_2}$, generate a vector normal to $h$, $\\vec{n}$, a point on the hyperplane, $\\vec{h_p}$, a vector $\\vec{l} = \\overrightarrow{\\vec{p_1}\\vec{p_2}}$.\n",
    "If $\\vec{l}\\cdot\\vec{n}=0$ there is not intersection.\n",
    "If $\\vec{l}\\cdot\\vec{n}\\neq0$ then define,<br>\n",
    "$d_0 = 0$<br>\n",
    "$d_1 = \\frac{\\vec{p_2} - \\vec{p_1}}{\\vec{l}}$<br>\n",
    "$d_q = \\frac{\\left(\\vec{h_p}-\\vec{p_1}\\right)\\cdot \\vec{n}}{\\vec{l}\\cdot\\vec{n}}$<br>\n",
    "If $d_q\\in[d_0,d_1]$ then there is an intersection, otherwise no intersection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import linalg as lin\n",
    "\n",
    "def vectorBetween(v1, v2):\n",
    "    return ((v1 - v2)/2)+v1\n",
    "\n",
    "def gs_cofficient(v1, v2):\n",
    "    return np.dot(v2, v1) / np.dot(v1, v1)\n",
    "\n",
    "def multiply(cofficient, v):\n",
    "    return map((lambda x : x * cofficient), v)\n",
    "\n",
    "def proj(v1, v2):\n",
    "    return multiply(gs_cofficient(v1, v2) , v1)\n",
    "\n",
    "def gs(v1, X): #This might be wrong, check here first if something breaks\n",
    "    orthonormals = numpy.zeros(size(X)+1)\n",
    "    for i in range(0):\n",
    "        vi = X[i] - proj(X[i], v1)\n",
    "        orthonomals[i] = (vi/lin.norm(vi))\n",
    "    orthonormals[size(X)] = v1 #normal to the hyperplane\n",
    "    return orthonormals\n",
    "\n",
    "#def intersect(h, vi, p):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
